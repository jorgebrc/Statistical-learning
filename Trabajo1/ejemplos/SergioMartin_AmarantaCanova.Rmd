---
title: "HOMEWORK 1"
author: "Sergio Martin & Amaranta Canova"
date: "2023-11-05"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## BREAST CANCER PREDICTION

Breast cancer, a formidable adversary in the realm of healthcare, affects countless lives worldwide. It is the most common cancer among women, with a wide array of intricately interconnected features defining its progression and diagnosis. The quest to enhance the early detection and understanding of this disease has never been more critical. In this project, we embark on a data-driven exploration, harnessing the power of unsupervised learning techniques to unravel the hidden insights within the breast cancer dataset.

Our journey is structured into five distinct phases:

**1. Data Preprocessing:** Before we dive deep into the data, we'll don our data detective hats to clean, prepare, and refine the dataset. This phase includes handling outliers and missing values (NA)

**2. Visualization Tools:** Visualizations are our compass, helping us navigate through the dataset's rich landscape. We'll use a range of visualization techniques to gain preliminary insights into the data, providing a visual map of its underlying structure, patterns, and relationships.

**3. Principal Component Analysis:** Armed with a better understanding of the data, we will venture into the realm of Principal Component Analysis (PCA). PCA is a dimensionality reduction technique that will allow us to highlight the most critical features that influence the variation within the dataset. It's a crucial step in simplifying complex data while preserving essential information.

**4. Factor Analysis:** Factor analysis is another powerful tool in our toolkit. It will help us uncover latent variables or factors that underlie the observed features. By discerning the fundamental drivers of breast cancer characteristics, we aim to identify potentially overlooked patterns and correlations.

**5. Clustering Tools:** Finally, we'll turn to clustering, where we let the data itself dictate its intrinsic groupings. Our goal is to identify clusters of patients with similar cancer characteristics, which could ultimately aid in more personalized treatment plans and prognostic predictions.

Through this project, we hope to contribute to the ongoing battle against breast cancer by unearthing the hidden connections and attributes within the data. Our ultimate objective is to empower healthcare professionals with improved diagnostic and treatment strategies, bringing us one step closer to a world with fewer uncertainties and brighter prospects for those affected by this formidable disease. Can we help with this study? Can we classify the data and find a relationship in which we can predict the patient's tumor type?

The dataset contains 569 samples of malignant and benign tumor cells

-   **ID Number**: A unique identification number for each sample.

-   **Diagnosis (M or B)**: This is the target variable. "M" stands for malignant, indicating cancerous tumors, while "B" stands for benign, indicating non-cancerous tumors.

-   **Radius (mean, se, worst)**: These features represent the mean, standard error, and worst (largest) radius of the cell nuclei. The radius is the average distance from the center of the nucleus to the points on its perimeter.

-   **Texture (mean, se, worst)**: These features represent the mean, standard error, and worst texture, which describes the variation in grayscale intensities of the pixels in the cell nucleus.

-   **Perimeter (mean, se, worst)**: These features represent the mean, standard error, and worst perimeter of the cell nuclei.

-   **Area (mean, se, worst)**: These features represent the mean, standard error, and worst area of the cell nuclei.

-   **Smoothness (mean, se, worst)**: These features represent the mean, standard error, and worst smoothness of the cell nuclei. Smoothness refers to the variation in radius lengths.

-   **Compactness (mean, se, worst)**: These features represent the mean, standard error, and worst compactness, which is calculated as the perimeter\^2 divided by the area.

-   **Concavity (mean, se, worst)**: These features represent the mean, standard error, and worst concavity, which quantifies the level of concave portions in the contour of the cell nuclei.

-   **Concave Points (mean, se, worst)**: These features represent the mean, standard error, and worst number of concave portions in the contour of the cell nuclei.

-   **Symmetry (mean, se, worst)**: These features represent the mean, standard error, and worst symmetry of the cell nuclei.

-   **Fractal Dimension (mean, se, worst)**: These features represent the mean, standard error, and worst fractal dimension, which quantifies the complexity of the cell nuclei's perimeter.

Load Libraries

```{r}
library(VIM)
library(Quandl)
library(VIM)
library(lubridate)
library(GGally)
library(factoextra)
library(quantmod)
library(tidyverse)
library(ggplot2)
library(ggExtra)
library(plotly)
library(mice)
library(igraph)
library(countrycode)
library(mclust)
library(cluster)
library(corrplot)
library(gridExtra)
library(Seurat)
library(rgl)
library(ggstatsplot)
library(rstantools)
library(outliers)
library(psych)
```

## Load data

```{r}
Data = read.csv("breast-cancer - copia.csv")
```

```{r}
dim(Data)
str(Data)
head(Data)
tail(Data)
summary(Data)
```

## Data Preproccesing

We are not interested on the patient_id as that is a random number assigned to each of it, so we remove that variable

```{r}
X = Data[,-1]
```

### Missing Values

```{r}
colSums(is.na(X))
#With that we already see that there are not missing values, but let's check making a plot 
aggr(X, numbers = T, sortVars = T, labels = names(X), cex.axis = .4, gap = 0.5, label = c("Missing data", "Patern"))



```

We have two missing values, one on the variable concave.points_mean and other on radius_se

As they are only two rows with 1 variable with an NA value , replace them by an estimate using random forest

```{r}
# Comlete NA values
imp = mice(X, method = "rf", m = 5)
# this method makes estimation of your Na values using random forest
X = complete(imp)
#Check that we don't have NA values now
colSums(is.na(X))
```

### Outliers

Make boxplot with each variable in order to identify the outliers

```{r}
par(mfrow=c(3,1))

ggplot(X) +aes(x =radius_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =texture_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =perimeter_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =area_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =smoothness_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =compactness_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concavity_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concave.points_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =symmetry_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =fractal_dimension_mean, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =radius_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =texture_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =perimeter_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =area_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =smoothness_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =compactness_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concavity_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concave.points_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =symmetry_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =fractal_dimension_se, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =radius_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =texture_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =perimeter_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =area_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =smoothness_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =compactness_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concavity_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =concave.points_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =symmetry_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()
ggplot(X) +aes(x =fractal_dimension_worst, color = "pink1", leyend = F) +geom_boxplot() + theme_minimal()





```

Most of them have some outliers. Remove them for having the data set cleaned

**REMOVE OUTLIERS:** By analyzing the previous boxplots, we can see from which values ​​in each variable there begin to be considerable outliers. For that reason, we are going to remove them

```{r}
# Remove rows with outliers
# Filter the data to remove outliers based on your criteria
X <- subset(
  X,
  !(radius_se > 2 |
    radius_mean > 25|
    texture_se > 3 |
    texture_mean > 32 |
    perimeter_se > 7.5 |
    perimeter_mean > 160 |
    smoothness_mean > 0.140 |
    area_mean > 1500 |
    concave.points_mean > 0.17 |
    compactness_mean > 0.25 |
    symmetry_mean > 0.25 |
    concavity_mean > 0.3 |
    radius_se > 1 |
    fractal_dimension_mean > 0.08 |
    texture_se > 2.75 |
    perimeter_se > 8 |
    area_se > 190 |
    smoothness_se > 0.015 |
    compactness_se > 0.55 |
    concavity_se > 0.1 |
    concave.points_se > 0.03 |
    symmetry_se > 0.04 |
    fractal_dimension_se > 0.01 |
    radius_worst > 30 |
    perimeter_worst > 200 |
    area_worst > 2500 |
    smoothness_worst > 0.20 |
    compactness_worst > 0.75 |
    concavity_worst > 1 |
    symmetry_worst > 0.5 |
    fractal_dimension_worst > 0.14
  )
)



```

## Visualization Tools

We are going to plot a pie in which we can see the frequency of cancer diagnosis

```{r}
#Create a pie chart
d_table = table(X$diagnosis)
# Create the prop tables and multiply by 100 because we want the results to be percentage
diagnosis.prop.table = prop.table(d_table)*100
# Now, transform the prop table into data frame
diagnosis.prop.df = as.data.frame(diagnosis.prop.table)
pielabels = sprintf("%s - %3.1f%s", diagnosis.prop.df[,1], diagnosis.prop.table, "%")

pie(diagnosis.prop.table, labels = pielabels, clockwise = T, col = terrain.colors(2), border = "grey", radius = 0.8) 
```

Here we see that there is more women that after diagnosis they get good results than the ones that get bad ones

Let's see if the variables are different depending on the diagnosis, make this only testing the feature that are means

```{r}
# We must transform the diagnosis variable into factor for making ggplots with it
X$diagnosis = as.factor(X$diagnosis)
```

```{r}
# Radius 
ggplot(X) + aes(x=radius_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("radius") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("RADIUS difference")
# Texture
ggplot(X) + aes(x=texture_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("texture") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("TEXTURE difference")
#Perimeter
ggplot(X) + aes(x=perimeter_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("perimeter") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("PERIMETER difference")
#area
ggplot(X) + aes(x=area_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("area") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("AREA difference")
#smoothness
ggplot(X) + aes(x=smoothness_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("smoothness") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("SMOOTHNESS difference")
#compactness
ggplot(X) + aes(x=compactness_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("compactness") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("COMPACTNESS difference")
#concavity
ggplot(X) + aes(x=concavity_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("concavity") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("CONCAVITY difference")
#concave.points
ggplot(X) + aes(x=concave.points_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("concave.points") + ylab("patients")+ scale_fill_manual(values = c("red", "seagreen2"))+ ggtitle("CONCAVE POINTS difference")
#symmetry
ggplot(X) + aes(x=symmetry_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("symmetry") + ylab("patients")+ scale_fill_manual(values = c("seagreen2", "red"))+ ggtitle("SYMMETRY difference")
#factal dimension
ggplot(X) + aes(x=fractal_dimension_mean, fill=diagnosis, alpha = 0.4) +geom_density()+xlab("fractal dimension") + ylab("patients")+ scale_fill_manual(values =c("seagreen2", "red"))+ ggtitle("FRACTAL DIMENSION difference")

```

See how most of the variables are distributed differently in relation to the type of tumor that the women have. We can stand out the concave.points, the concavity and the area

**Multiple Scatterplots**

Plot the correlation matrix:

```{r}
# We make the correlation with different methods in order to obtain exaclty the values that we want 
# Firstly , we are going to take into account the diagnosis variable, as we want to classify the data into malignant and benignant tumors, we need to work with variables that are correlated with the variable diagnosis. For make the correlation using diagnosis, the first thing to do is transform the variable into numerical
X$diagnosis = as.numeric(X$diagnosis)

spearman_cor=cor(X, method = "spearman")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(spearman_cor, method = "color", col = col(200), type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = T, addCoefasPercent = T, tl.cex = 0.3, number.cex = 0.5)

heatmap(cor(X))

correlations <- cor(X,method="pearson")
corrplot(correlations, number.cex = .9, method = "square", 
         hclust.method = "ward", order = "FPC",
         type = "full", tl.cex=0.8,tl.col = "black")



#make again the correlation without the values that are not correlated with diagnosis by both methods(pearson and spearman), those are : fractal_dimension_mean, texture_se and smoothness_se
X = X %>% dplyr :: select ( -fractal_dimension_mean, -texture_se, -smoothness_se)

spearman_cor=cor(X, method = "spearman")
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))

corrplot(spearman_cor, method = "color", col = col(200), type = "upper", tl.col = "black", tl.srt = 45, addCoef.col = T, addCoefasPercent = T, tl.cex = 0.5, number.cex = 0.5)

heatmap(cor(X))

correlations <- cor(X,method="pearson")
corrplot(correlations, number.cex = .9, method = "square", 
         hclust.method = "ward", order = "FPC",
         type = "full", tl.cex=0.8,tl.col = "black")


```

We can see that most of the variables are high correlated with each other, that's a good point because means that we can reach a prediction studying them

The highest correlations are between:

Perimeter_mean and radius_worst

area_worst and radius_worst;

texture_mean and texture_worst;

Perimeter_mean and area_worst

Let's show the plots for some of these highly correlated features. We will use ggplot, showing the scatter plot (in the two dimensions given by the selected features) for the clustered data (grouped by diagnosis)

But first transform the diagnosis variable into factor, save the variable and remove that column from the dataset in order to only work with numerical variables

```{r}
# Change into character before saving the variable
X$diagnosis = factor(X$diagnosis, levels = c("1","2"), labels = c("B", "M"))
                        
X$diagnosis = as.character(X$diagnosis)

#save it 
diagnosis = X$diagnosis
# Remove the diagnosis variable
X = X %>% dplyr :: select ( -diagnosis)
```

```{r}
g1 = ggplot(X) + aes(x = perimeter_mean, y = radius_worst, color= diagnosis)+ geom_jitter() + scale_color_manual(values = c("seagreen", "darkred"))+ theme_minimal()
g2 = ggplot(X) + aes(x = area_worst, y = radius_worst, color= diagnosis)+ geom_jitter() + scale_color_manual(values = c("seagreen", "darkred"))+ theme_minimal()
g3 = ggplot(X) + aes(x = texture_mean, y = texture_worst, color= diagnosis)+ geom_jitter() + scale_color_manual(values = c("seagreen", "darkred"))+ theme_minimal()
g4 = ggplot(X) + aes(x = perimeter_mean, y = area_worst, color= diagnosis)+ geom_jitter() + scale_color_manual(values = c("seagreen", "darkred"))+ theme_minimal()

grid.arrange(g1, g2, g3, g4, ncol=2)
```

In this plots we can see the huge correlation between those variables distinguishing by the diagnosis results

We are in dimension 30, we need to reduce the dimensionality. For that we are going to use 2 methods: PCA and FA

Let's use first the PCA in order to reduce dimensionality

## PCA

```{r}
pca = prcomp(X, scale=T)  
summary(pca)
```

Let's plot how many components we need to explain a good portion of the variability:

```{r}
fviz_eig(pca, addlabels=TRUE, ylim=c(0,60), geom = c("bar", "line"), barfill = "pink", barcolor="grey",linecolor = "red", ncp=10)+
labs(title = "Cancer All Variances - PCA",
         x = "Principal Components", y = "% of variances")
```

On this graph we see the % of variability explained by each component

```{r}
# Calculate the proportion of variance explained
pca_data_var<- pca$sdev^2
pv_data<-pca_data_var/sum(pca_data_var)
cum_pv<- cumsum(pv_data)
pv_table<- tibble(comp= seq(1:ncol(X)),pv_data,cum_pv)

#Let's plot the graph

ggplot(pv_table,aes(x=comp,y=cum_pv))+ geom_point(col= "red")+ geom_abline(intercept = 0.80,slope = 0)
```

With the help graph we can see that 4 components are sufficient for explaining more than 80% of the data.

It is a small number of components if we take into account that our data set has 30 variables to study.

Make a plot showing the correlation between variables and PCA

```{r}
#Get PCA variables
all_var <- get_pca_var(pca)
all_var
# Plot showing the relation
corrplot(all_var$cos2, is.corr=FALSE)
```

On this plot we can see how most of the variability is explained when we are in dimension 4 , that is, with 4 PCA's. As we have calculated previously, 4 components were the ones that explain more than 80% of variability, so we are going to plot those components, analyze them and seeing their contributions

```{r}
barplot(pca$rotation[,1], las=2, col="purple", cex.names = 0.35)
barplot(pca$rotation[,2], las=2, col="purple", cex.names = 0.35)
barplot(pca$rotation[,3], las=2, col="purple", cex.names = 0.35)
barplot(pca$rotation[,4], las=2, col="purple", cex.names = 0.35)

```

Analyzing the components: Contributions of the variables , for that we are going to use the function fviz_contrib : This function can be used to visualize the contribution of rows/columns from the results of Principal Component Analysis (PCA),

```{r}
p1 <- fviz_contrib(pca, choice="var", axes=1, fill="pink", color="grey", top=10)
p2 <- fviz_contrib(pca, choice="var", axes=2, fill="skyblue", color="grey", top=10)
p3 <- fviz_contrib(pca, choice="var", axes=3, fill="darkseagreen1", color="grey", top=10)
p4 <- fviz_contrib(pca, choice="var", axes=4, fill="bisque", color="grey", top=10)
grid.arrange(p1,p2,p3,p4,ncol=2)
```

On this plot we can see the top 10 variables that most contribute to each of our principal components,

Analyze of the components:

From all this different plots showing the characteristics of the PCA, we see that on the first component most of the variables have huge contributions excepting one: symmetry_se. The second has contributions from almost all the variables except from concave.points_mean, texture_worst and concave.points_worst. The third component has a huge contribution from all the standard error variables(especially the symmetry_se) , also has some contributions from the worst ones and very few ones from the means. Finally we can see how the fourth PCA has a strong contribution from 2 variables and very few contributions from the others and, casually, this 2 variables are of texture(texture_mean and texture_worst)

Lets now make a plot with the 2 first principal components, filling by diagnosis

```{r}
data.frame(z1=-pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2,label=diagnosis, color = diagnosis)) + geom_point(size=0.2) +
  labs(title="PCA", x="PC1", y="PC2") +
  theme_bw() + scale_color_manual(values =c("aquamarine4", "darkred"))+theme(legend.position="bottom") + geom_text(size=2, hjust=0.6, vjust=0, check_overlap = TRUE)    
       

```

Now, lets make the same but with the first 3 Principal components in 3D

```{r}
pc1 <- pca$x[, 1]
pc2 <- pca$x[, 2]
pc3 <- pca$x[, 3]
#for this plot Im going to create a variable substituing M-> red, B->green to distinguish on the 3d plot between malignant and benignant tumors
c_diag = diagnosis
c_diag = gsub("M", "darkred", c_diag)
c_diag = gsub("B", "aquamarine4", c_diag)
plot3d(pc1, pc2, pc3, type = "p", size = 10, col = c_diag, xlab = "PC1", ylab = "PC2", zlab = "PC3")
       
```

Remember that 3 components explain almost the 75% of the variability

In this last plots , the 3d and 2d ones, we can visualize the evident fact that we can classify the data distinguishing between malignant and benignant tumors using the Principal Components.

## Factor Analysis

Factor Analysis is an analytical tool that produces models that will reduce dimension (reduce columns). It creates linear combinations of factors in order to find which are the most important, because they 'capture' most of the variance of the dataset.

We make first the parallel analysis, this is one way to determine the number of factors in a data matrix that consist on examine the "scree" plot of the successive eigenvalues

```{r}
nafactor<-fa.parallel(X,fm = "ml",fa = "fa")
```

```{r}
nafactor$fa.values
```

Here we see the values of the factors of the parallel analysis , we know that the good factors are the ones that are greater than one, so let's compute them:

```{r}
sum(nafactor$fa.values>1)
```

so we obtain that 3 factors are good

**Run the Factor Analysis**

Four factors will be used since 3 are greater than 1 and there is one very near to 1, and let's also use the oblimin rotation because of the multicollinearity of the variables.nafactor\$fa.values

We will use the function fa(performs factor analysis on a given data set or correlation matrix and returns an object that contains information about the factor analysis results) with the factor method "ols" that it minimizes the entire residual matrix using an OLS procedure(minimize the sum of the squared residuals) using the empirical first derivative.

```{r}
fa1<-fa(X,nfactors = 4,rotate = "oblimin", scores = "regression", fm = "ols")
print(fa1)
```

**SS loadings**: if the number is grater than one is a good factor. Otherwise that factor doesn't explain well your data . We see that the 4 factors are good ones.

The four factors explain 78% of the variance: factor 1 for 35%, factor 2 for 23%, factor 3 for 13%, and factor 4 for 8%, also factors are poorly correlated let's use the orthogonal rotation (varimax)

This analysis will be done with the factor method "minchi", this method minimizes the sample size weighted chi square when treating pairwise correlations with different number of subjects per pair.

```{r}
fa2<-fa(X,nfactors = 4,rotate = "varimax", scores = "regression", fm="minchi")
print(fa2)
```

The factors explain almost the same variability with the method before, the root mean square of the residuals (RMSR) is 0.06 which is good, also the root mean square error approximation (RMSEA index) = 0.309 and the 90 % confidence intervals are 0.305 0.314. We can see know how the correlation has increased a lot.

### Interpretation

```{r}
par(mfrow=c(3,1))
barplot(fa2$loadings[,1], las=2, col="purple2", ylim = c(-1, 1), cex.names = 0.6 )
barplot(fa2$loadings[,2], las=2, col="purple2", ylim = c(-1, 1), cex.names = 0.6)
barplot(fa2$loadings[,3], las=2, col="purple2", ylim = c(-1, 1), cex.names = 0.6)
barplot(fa2$loadings[,4], las=2, col="purple2", ylim = c(-1, 1), cex.names = 0.6)





```

In this barplots we can see the contribution of each variable to each cluster, we can make a diagram for seeing the variables that contributes the most to each cluster

```{r}
fa.diagram(fa2, rsize = 0.97,main = "Factor analysis", digits=0.5, cex = 0.5 )
```

Conclusion of FA

Using Factor Analysis we can reduce the dimension of the data into 4 factors that explain almost the 80% of the variability, those factors have contributions from the variables of the data, and from the previews plots we can explain the contribution of each variable into each factor , the variables that have more influence in each factors we can see it perfectly on the diagram and are(in descendant contribution order):

F1 (MC1) = area_mean, area_worst, radius_worst, radius_mean, perimeter_mean, perimeter_worst, area_se, radius_se, perimeter_se, concave.points_mean, concave.points_worst, texture_mean, texture_worst

F2 (MC4) = compactness_se, concavity_se, fractal_dimension_se, compactness_worst, fractal_dimension_worst, concavity_worst, concave.points_se

F3(MC2) = smoothness_mean, smoothness_worst, compactness_mean, symmetry_mean, symmetry_worst

F4(MC3) = symmetry_se

**Regression analysis using the factor scores as the independent variable**

```{r}
#Let's leave the interpretation of the factors and their current names as they are right now and combine the dependent variable and the factor scores into a dataset.
#Create a variable that names M->0, B->1 and transform into numerical in order to make teh regression analysis
f_diag = diagnosis
f_diag = gsub("M", 0, f_diag)
f_diag = gsub("B", 1, f_diag)
f_diag = as.numeric(f_diag)
regression_data<-as.data.frame(cbind(f_diag,fa2$scores))
names(regression_data)<-c("diagnosis","F1","F2","F3","F4")
head(regression_data,10)




```

Divide the data into training and test sets before performing any analysis.

```{r}
set.seed(122)
#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(regression_data), replace=TRUE, prob=c(0.8,0.2))
train.data <- regression_data[sample, ]
test.data <- regression_data[!sample, ]

```

Investigate trends:

```{r}
ggplot(train.data,aes(y=diagnosis,x=F1))+
  geom_smooth(method="loess")+
  geom_smooth(method = "lm",col="red")+
  ggtitle("Relationship between diagnosis and F1")

ggplot(train.data,aes(y=diagnosis,x=F2))+
  geom_smooth(method="loess")+
  geom_smooth(method = "lm",col="red")+
  ggtitle("Relationship between diagnosis and F2")

ggplot(train.data,aes(y=diagnosis,x=F3))+
  geom_smooth(method="loess")+
  geom_smooth(method = "lm",col="red")+
  ggtitle("Relationship between diagnosis and F3")

ggplot(train.data,aes(y=diagnosis,x=F4))+
  geom_smooth(method="loess")+
  geom_smooth(method = "lm",col="red")+
  ggtitle("Relationship between diagnosis and F4")

```

We can see a good relation between the factors and the diagnosis in at least the 3 first factors

## Clustering

We initially select 2 clusters, as we want to predict if the patient's tumor is benignant or malignant

```{r}
# the best cluster is the one that give the fewer distances to the centroids
k = 2
fit = kmeans(scale(X), centers=k, nstart=1000)
groups = fit$cluster
barplot(table(groups), col="purple3")

```

```{r}
# clusplot
fviz_cluster(fit, data = X, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=diagnosis,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")




```

```{r}
t.kmeans <- eclust(X, "kmeans", stand=TRUE, k=2)
```

```{r}
d <- dist(scale(X), method="euclidean")  
sil = silhouette(groups, d)
plot(sil, col=1:2, main="", border=NA)
summary(sil)

# the same with factoextra
fviz_silhouette(t.kmeans)
```

### PAM

### Number of clusters by PAM

Lets select the optimal number of clusters for PAM

```{r}
#takes aprox 2 min
fviz_nbclust(scale(X), pam, method = 'silhouette', k.max = 10)
fviz_nbclust(scale(X), pam, method = 'gap_stat', k.max = 10, nboot = 500)
```

We see that with the silhouette method we reach the conclusion that 2 clusters are the best, however with the gap_stat method our conclusion is that 4 clusters are the best. So we are going to make the PAM clustering using 3 clusters

```{r}
#PAM clustering with 3 clusters
fit.pam <- eclust(scale(X), "pam", stand=TRUE, k=3, graph=F)

fviz_cluster(fit.pam, data = X, geom = c("point"), pointsize=1, ellipse.alpha=0.05)+
  theme_minimal()+geom_text(label=diagnosis,hjust=0, vjust=0,size=2,check_overlap = T)

```

From this plot, we can identify easily 3 different groups. The first cluster contains most of the malignant tumors, the second clusters contains mostly benignant tumors but also some malignant ones, we can interpret from that that those malignant tumors included on cluster 2 are not so advanced. Finally, the third cluster contains the rest of the benignant tumors.

### Optimal number of groups for kernel k means?

```{r}
fviz_nbclust(scale(X), kmeans, method = 'silhouette', k.max = 20, nstart = 1000)

fviz_nbclust(scale(X), kmeans, method = 'wss', k.max = 10, nstart = 1000)

```

We see that the best number of clusters to use in kernel k-means is 2, this is good for us as we want to classify between malignant and benignant, let's see the plot

### Kernel K-means

```{r}
library(kernlab)

fit.ker <- kkmeans(scale(as.matrix(X)), centers=2, kernel="rbfdot") # Radial Basis kernel (Gaussian)
# By default, Gaussian kernel is used
# By default, sigma parameter is estimated

centers(fit.ker)#coordinates of the centroids
size(fit.ker)#Number of obs in each cluster
withinss(fit.ker)

object.ker = list(data =X, cluster = fit.ker@.Data)

fviz_cluster(object.ker, geom = c("point"),pointsize=2, ellipse = F)+ theme_minimal()+scale_color_manual(values = c("darkred", "aquamarine4"))
fviz_cluster(object.ker, geom = c("point"), ellipse=
               T,pointsize=1, ellipse.apha = 0.1)+ theme_minimal()+geom_text(label=diagnosis,hjust=0, vjust=0,size=2,check_overlap =  T)+scale_color_manual(values =c("darkred", "aquamarine4"))

```

In this plot we can see perfectly 2 different groups. The first one contains almost all of the patients with malignant tumors and a few of benignant ones, we can interpret from this that the patients with benignant tumors included on cluster 1 are the ones that have to take few more checks in some time for studying the evolution of the tumor.

On the other hand, the second cluster contains all the benignant tumors and a few malignant, it is a very small proportion so we can ignore them or interpret that they may be very little advanced malignant tumors.

### Hierarchical clustering

This is a valuable tool for data exploration, visualization, and understanding relationships within a dataset. Its ability to create a hierarchy of clusters without requiring the prior specification of the number of clusters makes it particularly useful in exploratory data analysis and pattern discovery.

Important to decide distance between observations and linkage to join groups

We need to decide first the distance and linkage

```{r}
d = dist(scale(X), method = "euclidean")
hc <- hclust(d, method = "ward.D2") 

```

Visualization

We use k = 2 because we want to classificate the data in benignant and malignant tumors

**Classical dendogram:**

```{r}
hc$labels <- diagnosis
 #We need to take a sample of the data for making this clusterization
fviz_dend(x = hc, 
          k=2,
          palette = "jco", 
          rect = TRUE, rect_fill = TRUE, cex=0.5,
          rect_border = "jco")

```

It's very difficult to analyze that graph as we do not see properly the names of the two hierarchical groups

Let's use a phylogenic tree instead:

```{r}
fviz_dend(x = hc,
         k = 2,
          color_labels_by_k = TRUE,
          cex = 0.5,
          type = "phylogenic",
          repel = TRUE)+  labs(title="Malignant and Benignant Breast Cancer Tumors Cluster") + theme(axis.text.x=element_blank(),axis.text.y=element_blank())

```

On the phylogenic tree we can perfectly see how is separating the malignant and benignant tumors in two different groups. One containing all the patients with benignant tumor and other containing all the patients with the malignant ones. There are few malignant's in the cluster containing all the beningant's, but as they are a very small quantity we don't take it in consideration or interpret, as with the PAM clustering that those malignant tumors are not so advanced

### EM clustering

Expectation-Maximization clustering is like k-means but computes probabilities of cluster memberships based on probability distributions

Hence, the goal of the EM clustering then is to maximize the overall likelihood of the data, given the (final) clusters

```{r}
# takes aprox 2 min
res.Mclust <- Mclust(scale(X))
summary(res.Mclust)

# The clustering is probabilistic: for each country we don't have a unique group but the probabilities the country belongs to each of the groups
head(res.Mclust$z)

# Of course the tool assign the group with highest probability  
head(res.Mclust$classification)

fviz_mclust(object = res.Mclust, what = "BIC", pallete = "jco") +
  scale_x_discrete(limits = c(1:10))

```

```{r}
fviz_mclust(object = res.Mclust, what = "classification", geom = "point",
pallete = "jco")


fviz_mclust(object = res.Mclust, what = "classification", geom = "point",
pallete = "jco")+geom_text(label = diagnosis, size = 2, check_overlap = T)

```

Interpretation:

cluster 1: contains all malignant tumors, we can say that this are the most advanced tumors as is the only cluster that does not contain benignant ones

cluster 2: contains mostly malignant tumors and few of benignant ones, this can be interpreted as those benignant tumors may be wrong, for being sure , that patient should get a review again

clusters 3: mostly contains the patients with benignant tumors but also include few malignant ones, as is said previously in the project, this can be because not all the cancer tumors are equally advanced, and, in this case, this factor may contain the patients with less advanced tumors

clusters 4 & 5: contains all benignant tumors

```{r}
# Computes the adjusted Rand index comparing two classifications.
# The closer to 1 the more agreement
groups.hc = cutree(hc, k = 2)
adjustedRandIndex(res.Mclust$classification, fit.pam$clustering) 
adjustedRandIndex(fit.pam$clustering, t.kmeans$cluster) 
adjustedRandIndex(fit.pam$clustering, groups.hc)
adjustedRandIndex(t.kmeans$cluster, groups.hc)

```

```         
```

## CONCLUSION

We have done a very interesting study of the patients that have malignant or benignant tumors. The principal goal of this project was if we can classify (or predict) if the patient has a malignant or benignant tumors focus on the characteristics of the breast. We are very happy with the results since not only have we been able to achieve that goal, but we have also realized that these two groups can be subdivided into others dividing the malignant tumors into the ones that are more advanced and the ones that are less. And also the benignant ones in the results with more reliability and others that may be wrong. Breast cancer is a very serious problem that kills thousands of women a year, so we think that these types of studies are very necessary. And we hope they continue to do so