---
title: "HOMEWORK 1"
author: "Jorge Barcia & Paloma Núñez"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
# clear environment 
rm(list = ls())
``` 

```{r message=FALSE, warning=FALSE}
library(caret)
library(cluster)
library(corrplot)
library(countrycode)
library(dplyr)
library(factoextra)
library(FactoMineR)
library(gganimate)
library(GGally)
library(ggplot2)
library(gifski)
library(mclust)
library(psych)
library(rgl)
library(rmarkdown)
library(rnaturalearth)
library(rworldmap)
library(tidyr)
library(tidyverse)
``` 

# Data Source 
This project uses the [Life Expectancy Data](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who)

```{r}
data = read.csv("Life Expectancy Data.csv")
```

First, let's check the structure of our data.

```{r}
str(data)
``` 

We can see that we have both numerical and categorical features.
We can also see that we need to convert categorical columns to factors.

```{r} 
data = mutate(data, across(c(Country, Status),as.factor))
```

1. **country**: 
    - **Description**: The name of the country for which the life expectancy and other health indicators are reported.
    
2. **year**: 
    - **Description**: The year in which the data was recorded.
    
3. **status**: 
    - **Description**: The status of the country in terms of development, indicating whether the country is classified as developed or developing.
    
4. **life_expectancy**: 
    - **Description**: The average life expectancy of people in a country for a given year. 
    
5. **adult_mortality**: 
    - **Description**: The number of adult deaths (per 1,000 adults) due to various factors such as disease, accidents, and other causes. 
    
6. **infant_deaths**: 
    - **Description**: The number of infant deaths (per 1,000 live births) in a given year. 
    
7. **alcohol**: 
    - **Description**: The average amount of alcohol consumed per person (in liters of pure alcohol) in a given year.
    
8. **percentage_expenditure**: 
    - **Description**: The percentage of a country's GDP spent on healthcare in a given year.
    
9. **hepatitis_b**: 
    - **Description**: The percentage of infants that were vaccinated against Hepatitis B.
  
10. **measles**: 
    - **Description**: The number of reported cases of measles per 1,000 people in a given year. 
    
11. **BMI (Body Mass Index)**: 
    - **Description**: The average BMI (Body Mass Index) of the population of a given country, which is used to determine whether individuals are underweight, normal weight, overweight, or obese.
  
12. **under-five_deaths**: 
    - **Description**: The number of deaths per 1,000 live births of children under the age of 5.
  
13. **polio**: 
    - **Description**: The percentage of infants vaccinated against polio in a given year. 
  
14. **total_expenditure**: 
    - **Description**: The total amount of health expenditure (in US dollars) per person in a given year. 
  
15. **diphtheria**: 
    - **Description**: The percentage of infants vaccinated against Diphtheria in a given year. 
  
16. **hiv_aids**: 
    - **Description**: The number of deaths (per 100,000 people) due to HIV/AIDS in a given year. 
  
17. **gdp**: 
    - **Description**: The Gross Domestic Product (GDP) of a country per capita in USD. 
  
18. **population**:
    - **Description**: The total population of the country for the given year. 
  
19. **thinness_1_19_years**: 
    - **Description**: The percentage of the population between 1 and 19 years old that is considered undernourished or thin (measured by BMI). 

20. **thinness_5_9_years**: 
    - **Description**: The percentage of the population between 5 and 9 years old that is considered undernourished or thin (measured by BMI). 

21. **income_composition_of_resources**: 
    - **Description**: A composite index of income composition of resources, which measures the percentage of income that is available for investing in healthcare and other welfare. 

22. **schooling**: 
    - **Description**: The average number of years of schooling received by people in the country. 

## Handling NA Values. Visualizing missing values in each column.

Let's see a summary of our data to have a first glance of the NA Values and other important information.

```{r} 
summary(data)
```

In order to see clearly the Number NA values and where they are located.

```{r}
sum(is.na(data))

sapply(data, function(x) sum(is.na(x)))
```

Let's plot this info.

```{r} 
na_counts = sapply(data, function(x) sum(is.na(x))) 
aux_df = data.frame(Column = names(na_counts), NA_Count = na_counts)

ggplot(aux_df) +
  aes(x = reorder(Column, -NA_Count), y = NA_Count) +
  geom_bar(stat = "identity", fill = "pink") + 
  labs(title = "Number of NA Values", x = "Columns", y = "Count of NA Values") + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1)) 

```

As our missing values are in numerical variables, we will try imputation methods.

```{r}
# loop through each column
for (col in names(data)) {
  if (is.numeric(data[[col]])) {
    # impute missing values with the column mean
    data[[col]][is.na(data[[col]])] = mean(data[[col]], na.rm = TRUE)
  }
}

```

## Feature Engineering.

```{r}
# create a life expectancy category based on cut-offs
data$Life_Expectancy_Category = cut(data$Life.expectancy, 
                                     breaks = c(0, 50, 70, 100),
                                     labels = c("Low", "Medium", "High"))
# average vaccination rates.
data$AVG_vac = rowMeans(cbind(data$Hepatitis.B,data$Polio,data$Diphtheria))
# we remove individual vacc rates 
data$Hepatitis.B = NULL
data$Polio = NULL
data$Diphtheria = NULL
# avoid multicollinearity with BMI
data$thinness..1.19.years = NULL
data$thinness.5.9.years = NULL


```

## Visualization Tools. Exploring distributions and relationships.

`Life Expectancy` map

```{r}
lex_data = select(data,Country, Year, Life.expectancy)
world = ne_countries(scale = "medium", returnclass = "sf")
lex_data = rename(lex_data,name = Country)

world_gdp = left_join(world,lex_data, by = "name")
  
ggplot(world_gdp) +
  geom_sf(aes(fill = Life.expectancy)) +
  scale_fill_viridis_c(option = "turbo", na.value = "grey50", name = "Life Expectancy",direction = -1) +
  labs(title = "Life Expectancy by Country") +
  theme_minimal() 
```

  **Correlation analysis**

Using a correlation plot `corrplot`, we visualize the strength and direction of these associations. We also added a heat map to see better what pairs are related.

```{r}
numeric_cols =select(data,-Country, -Status,-Life_Expectancy_Category)

correlations = cor(numeric_cols,method="spearman")
corrplot(correlations, number.cex = .9, method = "square", 
         hclust.method = "ward", order = "FPC",
         type = "lower", tl.cex=0.8,tl.col = "black")
heatmap(cor(numeric_cols))
```

We can see that we have some strong positive correlation along the diagonal and strong negative correlation in the corner. 

Let's see the distribution of `Life Expectancy` for developed and developing countries. 

```{r}
ggplot(data) +
  aes(x = Life.expectancy, fill = Status) +  
  geom_histogram(bins = 30, color = "black") +
  labs(title = "Distribution of Life Expectancy by Development Level",
       x = "Life Expectancy", y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("lightgreen", "lightpink")) +
  transition_states(Status, transition_length = 0.5, state_length = 1) +
  labs(subtitle = "Development Level: {closest_state}")
```

Now, we will see how is `Schooling` correlated with `Life Expectancy`, and where are the developed countries allocated.

```{r}
ggplot(data) +
   aes(x = Schooling, y = Life.expectancy, color = Status) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  theme_minimal() +
  labs(title = "Life Expectancy vs Schooling",
       x = "Schooling (Years)", y = "Life Expectancy", color = "Status") +
  theme(plot.title = element_text(hjust = 0.5))
```

We'll see how the `Life Expectancy` has shifted to higher values over the years. 

```{r}
data$Year = as.integer(data$Year)
ggplot(data) +
  aes(x = Life.expectancy, fill = Life_Expectancy_Category) +
  geom_histogram(binwidth = 2, color = "black", alpha = 0.7) +
  labs(title = "Distribution of Life Expectancy: Year {frame_time}",
       x = "Life Expectancy", y = "Count", fill = "Life Expectancy") +
  theme_minimal() +
  transition_time(Year) +
  ease_aes('linear')
```

We can see how the red values slowly disappear, and how the plot shifts to the right.

## Outlier Detection 

As we are using a highly reliable dataset, outliers may not necessarily be errors or irrelevant data points. Instead, they might represent meaningful extreme values so we are going to only to erase the outliers that are impossible, for example, the categories that are over 1000 and have +1000 values.

```{r}
data = data[data$infant.deaths <= 1000, ]
data = data[data$Measles <= 1000, ]
data = data[data$under.five.deaths <= 1000, ]
```

  **Using boxplots**

Now, we will check for outliers that could ruin our analysis. Note that we can only have outliers in our numerical features.

```{r} 
numeric_cols =select(data,-Country, -Status,-Life_Expectancy_Category)
numeric_cols_long = pivot_longer(numeric_cols, cols = everything(), names_to = "Variable", values_to = "Value") 
ggplot(numeric_cols_long) +
  aes(x = Variable, y = Value, fill = Variable) +
  geom_boxplot(outlier.color = "red", outlier.size = 1) + 
  labs(title = "Outliers in Numerical Columns", x = "Variables", y = "Value") + 
  theme_minimal() + theme(axis.text.x = element_text(hjust = 0.5), legend.position = "none")+
  facet_wrap(~ Variable, scales = "free")
```

**Using 3-sigma rule**

```{r}
detect_outliers_3sigma = function(x) {
  mean_x = mean(x, na.rm = TRUE)
  sd_x = sd(x, na.rm = TRUE)
  lower_bound = mean_x - 3 * sd_x
  upper_bound = mean_x + 3 * sd_x
  return(x < lower_bound | x > upper_bound)
}
outliers = data.frame(
  Year = detect_outliers_3sigma(data$Year),
  Life.expectancy = detect_outliers_3sigma(data$Life.expectancy),
  Adult.Mortality = detect_outliers_3sigma(data$Adult.Mortality),
  infant.deaths = detect_outliers_3sigma(data$infant.deaths),
  Alcohol = detect_outliers_3sigma(data$Alcohol),
  percentage.expenditure = detect_outliers_3sigma(data$percentage.expenditure),
  Measles = detect_outliers_3sigma(data$Measles),
  BMI = detect_outliers_3sigma(data$BMI),
  under.five.deaths = detect_outliers_3sigma(data$under.five.deaths),
  Total.expenditure = detect_outliers_3sigma(data$Total.expenditure),
  HIV.AIDS = detect_outliers_3sigma(data$HIV.AIDS),
  GDP = detect_outliers_3sigma(data$GDP),
  Population = detect_outliers_3sigma(data$Population),
  Income.composition.of.resources = detect_outliers_3sigma(data$Income.composition.of.resources),
  Schooling = detect_outliers_3sigma(data$Schooling),
  AVG_vac = detect_outliers_3sigma(data$AVG_vac)
)
outlier_counts = sapply(outliers, sum)
outlier_counts
```

We can not be sure if the data is wrong or a extreme reality so we are going to remove every outlier.

```{r}
data = data[!apply(outliers, 1, any), ]
```

## PCA

Preprocessing for the PCA.

```{r}
PCA_data = data
# remove the features we created to do the plots 
PCA_data$Life_Expectancy_Category = NULL
# remove non numerical data
PCA_data$Country = NULL
PCA_data$Year = NULL
PCA_data$Status = NULL
```

Perform PCA.

```{r}
pca = prcomp(PCA_data, scale. = TRUE)

# view PCA summary
summary(pca)

fviz_eig(pca, addlabels = TRUE, ylim = c(0, 45), geom = c("bar", "line"), 
         barfill = "pink", barcolor = "grey", linecolor = "red", ncp = 10) +
  labs(title = "PCA - Variance Explained", x = "Principal Components", y = "% of Variance")
```

```{r}
# visualize variable contributions to the first component
fviz_contrib(pca, choice = "var", axes = 1, fill = "pink", color = "grey", top = 10)
# visualize variable contributions to the second component
fviz_contrib(pca, choice = "var", axes = 2, fill = "skyblue", color = "grey", top = 10)
# visualize variable contributions to the second component
fviz_contrib(pca, choice = "var", axes = 3, fill = "lightgreen", color = "grey", top = 10)
# visualize variable contributions to the second component
fviz_contrib(pca, choice = "var", axes = 4, fill = "lightcoral", color = "grey", top = 10)
```

We can see that PC1 has more variables whose contribution is small, while in PC2, PC3 and PC4, they all have one or two more contributing variables, for example, in PC4 `Total Expenditure` has almost 50% of the contribution.

Note that with 4 components we are able to explain 66% of the variability.

# Rank countries by PCA

PC1
```{r}
names = data$Country
ordered_indices = order(pca$x[, 1])

best_performers = data.frame(
  Country = data$Country[ordered_indices][(length(names) - 5):length(names)][rev(seq_len(6))],
  Year = data$Year[ordered_indices][(length(names) - 5):length(names)][rev(seq_len(6))]
)

worst_performers = data.frame(
  Country = data$Country[ordered_indices][1:10],
  Year = data$Year[ordered_indices][1:10]
)

print(worst_performers)

print(best_performers)

```
PC2
```{r}
names = data$Country
ordered_indices = order(pca$x[, 2])

worst_performers = data.frame(
  Country = data$Country[ordered_indices][(length(names) - 5):length(names)],
  Year = data$Year[ordered_indices][(length(names) - 5):length(names)]
)

best_performers = data.frame(
  Country = data$Country[ordered_indices][1:10],
  Year = data$Year[ordered_indices][1:10]
)

print(worst_performers)

print(best_performers)

```

Let's now visualize the top-10 Countries contributions to first component:

```{r}
names_order = names[order(get_pca_ind(pca)$contrib[,1],decreasing=T)]
combined_labels = paste(names_order, data$Year[order(get_pca_ind(pca)$contrib[, 1], decreasing = TRUE)], sep = " - ")

fviz_contrib(pca, choice = "ind", axes = 1, top=10)+scale_x_discrete(labels=combined_labels)
```

From this we can see that the worst Countries are the ones that are contributing more to the total Variance, and those contributions are not equally distributed through the observations.

Let's see if PC can help use to predict if a country is developed or developing.

```{r}
# 2D Data Frame
pca_plot_data = data.frame(
  PC1 = -pca$x[, 1],  
  PC2 = pca$x[, 2],
  Category = data$Life_Expectancy_Category
)
# 2D plot 
ggplot(pca_plot_data, aes(x = -PC1, y = PC2, label = Category, color = Category)) +
  geom_point(size = 0.5) +
  labs(title = "PCA - Life Expectancy Data", x = "Principal Component 1", y = "Principal Component 2") +
  theme_bw() +
  scale_color_manual(values = c("darkred", "darkorange","darkgreen")) +
  theme(legend.position = "bottom") +
  geom_text(size = 2, hjust = 0.6, vjust = 0, check_overlap = TRUE)
```

With PC1 and PC2 we can see that we may be able to predict it. Now, we add PC3 and PC4 (as size) to see if this helps.

```{r}
# 4D data frame
pca_3d_data = data.frame(
  PC1 = pca$x[, 1],
  PC2 = pca$x[, 2],
  PC3 = pca$x[, 3],
  PC4 = pca$x[, 4],
  Category = data$Life_Expectancy_Category
)
# color
category_colors = ifelse(pca_3d_data$Category == "High", "darkgreen",
                          ifelse(pca_3d_data$Category == "Medium", "darkorange", "darkred"))

# size
pca_3d_data$PC4_normalized = scales::rescale(pca_3d_data$PC4)
# 4D plot 
open3d()
plot3d(
  x = pca_3d_data$PC1,
  y = pca_3d_data$PC2,
  z = pca_3d_data$PC3,
  col = category_colors,
  type = "s",
  size = pca_3d_data$PC4_normalized * 5,
  xlab = "PC1",      
  ylab = "PC2",      
  zlab = "PC3",      
  main = "3D PCA Plot of Life Expectancy Data"
)
legend3d("topright", legend = c("Low", "Medium","High"), 
         col = c("darkred", "darkorange","darkgreen"), pch = 16, cex = 1)
# for rendering in html format
rglwidget()
```

From the 3D plot, we see that we could predict the `Life Expectancy` with PC1, PC2, PC3 and PC4, 
and taking into account that with PC4 we can explain almost up to 66% of the variance, we can say that we have successfully reduced the dimensionality from 17 to only 4.

## Factor Analysis

Factor analysis is a statistical method used to identify underlying relationships between variables by grouping them into "factors," which represent shared patterns making it useful for exploring and modeling complex datasets.

FA data preproccesing.

```{r}
fa_data = data
# remove the features we created 
fa_data$Life_Expectancy_Category = NULL
# remove non numerical data and unimportant data for the FA
fa_data$Country = NULL
fa_data$Year = NULL
fa_data$Status = NULL
# check for Multicollinearity
cor_matrix = cor(fa_data)
high_corr = findCorrelation(cor_matrix, cutoff = 0.9)  
fa_data = fa_data[, -high_corr]
```

Let's check the scree plot:

```{r}
screepl = fa.parallel(fa_data,fm = "ml",fa = "fa")
```

We perform FA and plot the loadings and Uniqueness to better understand what we have. 

```{r}
fa_result = factanal(fa_data, factors = 6, rotation="varimax", scores="Bartlett", lower = 0.01)

# save this for conclusions of clusters 
faRes = fa_result

fa_result

cbind(fa_result$loadings, "Uniqueness" = fa_result$uniquenesses)
```

Loading close to +-1 means that that variable is really well explained by that specific factor. A loading close to 0 means that is bad explained. This indicates then that `Life expectancy` is really well explained by factor 1 but not so good by factor 4.
If we focus on the uniqueness, we can see that, for example, the proportion of variance which we can not explain from `Life Expectancy` is 8% 
However, for `Population` it is 89%, which means that it can not be explained by a linear combination of these factors. We see that our model with 4 factors can explain 50% of the variance. Therefore, we will need to add more factors for better representing the model, which is not really ideal. So let's do something to improve our situation.

There are some variables that seem to be behave independently on the dataset like `Population`and `Measle` among others, as even using 6 factors their uniqueness is still high. We are going to try again without these variables.

```{r}
fa_data$AVG_vac = NULL
fa_data$Population = NULL
fa_data$Total.expenditure = NULL
fa_data$Measles = NULL

fa_result2 = factanal(fa_data, factors = 3, rotation="varimax", scores="Bartlett", lower = 0.01)

fa_result2

cbind(fa_result2$loadings, "Uniqueness" = fa_result2$uniquenesses)
```

We now see that without these variables, we get better proportion of variance explained (61%) with less Factors 3.

## Conclusion of FA

In our factor analysis, we observed that certain variables, such as `Population` and `Measles`, consistently showed high uniqueness values across various models. This suggests that these variables behave independently and are not well-explained by a linear combination of factors derived from the other variables in the dataset. Consequently, we excluded these variables to improve model coherence and the overall explained variance.
After removing these independent variables, our refined model with 3 factors was able to explain 61% of the total variance in the dataset, a notable improvement with fewer factors. This indicates a more efficient model structure, with cohesive underlying patterns among the remaining variables.
For the specific purpose of predicting `Life Expectancy`, we found that using only 2 factors in the model could explain up to 94% of the variance in this variable.

## Clustering tools

Some Data preprocessing.

```{r}
Category = data$Life_Expectancy_Category
# the clusters are at their best when the distances to the centroids is minimal 
X = select(data,-Country,-Year,-Status,-Life_Expectancy_Category)
k = 2
X = scale(X)
```

We are going to start by determining the optimal number of clusters.

```{r}
# elbow Method
fviz_nbclust(X, kmeans, method = "wss") + 
  ggtitle("Elbow Method for Optimal Clusters")

# the Elbow Method do not leave it completely clear so let use other
# silhouette Method
fviz_nbclust(X, kmeans, method = "silhouette") +
  ggtitle("Silhouette Method for Optimal Clusters")
```

Perform K-means Clustering with 2 Clusters.

```{r}
set.seed(123)
kmeans_result = kmeans(X, centers = k, nstart = 1000)

cluster = kmeans_result$cluster
barplot(table(cluster), col="pink")
```






```{r}
# clusplot without names as they are unreadable 
fviz_cluster(kmeans_result, data = X, geom = "point", ellipse.type = "norm") +
  labs(title = "K-means Clustering with 2 Clusters on Life Expectancy Data") +
  theme_minimal()
```

```{r}
fit.kmeans = eclust(X, "kmeans", stand=TRUE, k=k)
```

Let's check the quality of our clusters. 

```{r}
fviz_silhouette(fit.kmeans)
```


Now, we see wwere are this clusters allocated in our original dataset to retrieve some knowledge.

```{r}
# add cluster data
data$Cluster = kmeans_result$cluster

# calculate summary statistics for each cluster
cluster_summary = aggregate(. ~ Cluster, data = data, FUN = mean, na.rm = TRUE)
cluster_summary = select(cluster_summary,-Country,-Year,-Status,-Life_Expectancy_Category)
print(cluster_summary)
```

From this table, we see the differences between the mean values of countries in cluster 1 and 2.
In the clustering analysis of the Life Expectancy dataset, `Cluster 2` shows higher averages for positive indicators such as `life expectancy, GDP, and schooling`, suggesting that countries in this cluster tend to have better socioeconomic and health conditions. Conversely, negative indicators such as `adult mortality, infant deaths, and HIV/AIDS prevalence` are lower in Cluster 2, indicating better healthcare outcomes. In `Cluster 1`, these positive indicators are lower on average, while negative health indicators are higher, implying that countries in this cluster may face more significant health and economic challenges.

From this analysis we can also confirm our original suspect that there are not really important differences between `Population` from high developed countries and low developed ones. Other interesting insight is that developed countries drink more Alcohol, `total expenditure` and `Income.composition.of.resources` are not that far apart between developed and developing countries, suggesting that it may be the cases that even health care has less quality it has roughly the same cost.

As one of our crucial insights, what we have realized in this step is that even that the means of `Measles` are far apart one group from an other. The FA did not show important linear relations and seemed to be independent.
After investigating this phenomenon, we arrive to the conclusion that Measles incidence rates are more influenced by factors like periodic vaccination campaigns or public health policies, rather than forming a predictable, linear relationship with socioeconomic indicators.

This is why Factor Analysis did not identify Measles as strongly associated with any particular factor, even though clusters reveal distinct differences in Measles rates.

Let's confirm our theory by checking what are the most influential observations.

In order to check our theory, we need to measure the influence of each data point for this objective. We are using Cook's distance 
```{r}
factor_scores = faRes$scores
# make linear regression with the Scores 
model = lm(data$Measles ~ factor_scores)
# calculate distance
cooksd = cooks.distance(model)
# sort
ordered_cooksd = sort(cooksd, decreasing = TRUE)
# top 10
top_10_influential_indices = order(cooksd, decreasing = TRUE)[1:10]

# select country and year
top_10_countries_and_years = data[top_10_influential_indices, c("Country", "Year")]

print(top_10_countries_and_years)
```

We are ordering them using Cook’s Distance so we can get an idea of the most influential observations.
Observe that some countries like France or Japan, that are placed in the cluster of high developed, are among this list. This confirms our theory that Measles are not following the expected linear relation 


### PAM 

Now we are going to try new ways of clustering.

```{r}
# PAM clustering with 3 clusters
fit.pam = eclust(scale(X), "pam", stand=TRUE, k=2, graph=F)

fviz_cluster(fit.pam, data = X, geom = c("point"), pointsize=1)+
  theme_minimal()+scale_fill_brewer(palette="Paired")
```

Let's check if something has changed. 

```{r}
adjustedRandIndex(fit.kmeans$cluster, fit.pam$clustering)
fviz_nbclust(scale(X), pam, method = 'silhouette', k.max = 10)
```

Almost 90% of agreement and same number of optimal clusters.

### Visualization of Clusters in a map
  
```{r}
map_data = data.frame(
  country = data$Country,                  
  value = kmeans_result$cluster 
)
map_data$country = countrycode(map_data$country, 'country.name', 'iso3c')

matched_map = joinCountryData2Map(map_data, joinCode = "ISO3", nameJoinColumn = "country")

mapCountryData(
  matched_map, 
  nameColumnToPlot = "value", 
  missingCountryCol = "grey50",         
  borderCol = "lightblue",               
  catMethod = "pretty",                
  colourPalette = c("#FFB3BA","#BAFFC9"),             
  mapTitle = "Clusters",              
  lwd = 1                             
)
```

If we compare it with the first map we created with Life Expectancy, we can see that they highlight similar countries.

### EM clustering

Let's try clustering based on probability distributions.

```{r}
res.Mclust = Mclust(scale(X))
summary(res.Mclust)


fviz_mclust(object = res.Mclust, what = "BIC", pallete = "jco") +
  scale_x_discrete(limits = c(1:10))
```

We have an interesting optimal number of clusters, as the method is suggesting 8 Clusters as the optimal number.

```{r}
# visualize the 8 clusters 
fviz_mclust(object = res.Mclust, what = "classification", geom = "point",
            pallete = "jco")
```

As the number of optimal cluster has changed, we expect to have low similarity with the PAM clusterization. Let's check it.

```{r}
adjustedRandIndex(res.Mclust$classification, fit.pam$clustering) 
```

Now, we visualize how the 8 clusters would look like in a map.

```{r}
groups.mclust = res.Mclust$classification                          

map = data.frame(
  country = data$Country,         
  value = groups.mclust    
)

map$country = countrycode(map$country, 'country.name', 'iso3c')

# join clustering data with world map
matched = joinCountryData2Map(
  map, 
  joinCode = "ISO3", 
  nameJoinColumn = "country"
)
# plot the map
mapCountryData(
  matched,
  nameColumnToPlot = "value",         
  missingCountryCol = "grey50",        
  borderCol = "lightblue",            
  catMethod = "pretty",              
  colourPalette = c(
  "#FFB3BA",  
  "#FFDFBA",  
  "#FFFFBA",  
  "#BAFFC9",  
  "#D5BAFF",  
  "#FFC9DE",  
  "#C9FFFC"   
),             
  mapTitle = "Clusters",              
  lwd = 1                             
)
```

We have not found any more interesting conclusions like the `Measles` from this other forms of clusterization.

## CONCLUSION

We have uncovered interesting insights and significantly enhanced our understanding of this dataset, particularly the relationships between key health and socioeconomic indicators. Through the application of Unsupervised Learning techniques, such as Factor Analysis, Principal Component Analysis and Clustering, we have identified patterns and influential observations that challenge assumptions about linear relationships, such as the unexpected behavior of measles rates in highly developed countries. This analysis have not only improved our knowledge of the dataset, but also strengthened our skills in leveraging unsupervised learning methods for exploratory data analysis.



# Bibliography 
```{r}
# map
# https://r-spatial.org/r/2018/10/25/ggplot2-sf.html
# animations
# https://gganimate.com/
# 3D graph
# https://r-graph-gallery.com/3d_scatter_plot.html
# embed 3D graph to html
# https://stackoverflow.com/questions/63595786/rmarkdown-how-to-embed-an-3d-plot
# identify influential data points
# https://www.statology.org/how-to-identify-influential-data-points-using-cooks-distance/#:~:text=A%20data%20point%20that%20has,considered%20to%20be%20an%20outlier.
```
We have also used the notebooks provided in class.



